---
title: "Final Project- Group 7"
author: "Azaz Zaman, Shakiba Kazemian, Erick David Gutierrez Benites" 
date: "2024-11-17"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Set a seed for reproducibility
```{r}
set.seed(123)
```


## Log-Loss Objective Function:
```{r}
log_loss <- function(beta, X, y) {
  p_i <- 1 / (1 + exp(-X %*% beta))
  -sum(y * log(p_i) + (1 - y) * log(1 - p_i))
}

```


## Gradient Descent Optimization:
The following codes represents an optimization algorithm used to minimize the cost (or loss) function of a model by iteratively adjusting its parameters (in our case "beta") in the direction that reduces the error.

The gradient is the vector of partial derivatives of the loss function with respect to the model's parameters. It tells you how the loss function changes as you change the parameters. In simple terms, the gradient indicates the direction of the steepest increase in the loss function.

This process is repeated for a set number of iterations (or until convergence) to gradually improve the model's parameters by minimizing the loss or objective function.

The algorithm stops when the change in the loss function between iterations is very small (below a threshold, typically called tolerance), or when the maximum number of iterations is reached.

We have set our tolerance level is at 0.01.

```{r}
gradient_descent <- function(X, y, learning_rate = 0.01, max_iter = 1000, tol = 0.01) {
  beta <- runif(ncol(X))  # Random initialization with length equal to number of columns in X
  loss_old <- log_loss(beta, X, y)
  
  for (i in 1:max_iter) {
    p_i <- 1 / (1 + exp(-X %*% beta))  # Calculate probabilities at each iteration
    gradient <- -t(X) %*% (y - p_i)  # Compute the gradient
    beta <- beta - learning_rate * gradient  # Update beta
    
    loss_new <- log_loss(beta, X, y)
    if (abs(loss_old - loss_new) < tol) break  # Convergence check
    loss_old <- loss_new
  }
  
  return(beta)
}


```

## Initial Coefficients Using Least Squares:
```{r}
predict_class <- function(X, beta) {
  p_i <- 1 / (1 + exp(-X %*% beta))
  return(ifelse(p_i > 0.5, 1, 0))
}


```


## Bootstrap Confidence Intervals:
```{r}
bootstrap_ci <- function(X, y, beta_hat, B = 20, alpha = 0.05) {
  n <- nrow(X)
  boot_coefs <- matrix(NA, nrow = B, ncol = length(beta_hat))
  
  for (b in 1:B) {
    sample_indices <- sample(1:n, n, replace = TRUE)
    X_boot <- X[sample_indices, ]
    y_boot <- y[sample_indices]
    boot_beta <- gradient_descent(X_boot, y_boot)  
    boot_coefs[b, ] <- boot_beta
  }
  
  ci <- apply(boot_coefs, 2, quantile, probs = c(alpha / 2, 1 - alpha / 2))
  return(ci)
}


```

## Prediction and Confusion Matrix:
```{r}
confusion_matrix <- function(y_true, y_pred) {
  tp <- sum(y_true == 1 & y_pred == 1)
  tn <- sum(y_true == 0 & y_pred == 0)
  fp <- sum(y_true == 0 & y_pred == 1)
  fn <- sum(y_true == 1 & y_pred == 0)
  
  prevalence <- mean(y_true)
  accuracy <- (tp + tn) / length(y_true)
  sensitivity <- tp / (tp + fn)
  specificity <- tn / (tn + fp)
  fdr <- fp / (tp + fp)
  dor <- (tp / fn) / (fp / tn)
  
  return(list(
    Prevalence = prevalence,
    Accuracy = accuracy,
    Sensitivity = sensitivity,
    Specificity = specificity,
    FalseDiscoveryRate = fdr,
    DiagnosticOddsRatio = dor,
    ConfusionMatrix = matrix(c(tp, fn, fp, tn), nrow = 2, byrow = TRUE,
                             dimnames = list("Actual" = c("1", "0"), "Predicted" = c("1", "0")))
  ))
}


```


