---
title: "`TrainPredict` Vignette"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{`TrainPredict` Vignette}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

## Introduction

The purpose of this package is to simplify the process of sampling data, training logistic regression models, and making binary classification in R. In this vignette, you will find an introduction to the main functions of the `TrainPredict` package:

-   `train_test_sampling`: Split your dataset into training and test sets.
-   `lr`: Train a logistic regression model.
-   `predict_test`: Make predictions on the test dataset.
-   `predict_new`: Make predictions on new data using a model trained with function `lr`.

## Installation

To install this package you have to use the function `install_github`, the organization name is `AU-R-Programming` and the repository is `Final_Project_Group_7`

``` r
devtools::install_github("AU-R-Programming/Final_Project_Group_7")
```

## Load the Package

Let's start by loading the `TrainPredict` package:

```{r}
library(TrainPredict)
```

## Example Dataset

For demonstration purposes, we will use the built-in `iris` dataset. We'll modify it slightly to create a binary outcome suitable for logistic regression.

```{r}
# Preparing the dataset.
data("iris")
iris_binary <- iris
iris_binary$Virginica<-iris_binary$Species=='virginica'
#Creating a binary variable called 'Virginica' that indicates if the specie is Virginica or not.
head(iris_binary)

```

## Using `train_test_sampling`

The `train_test_sampling` function is designed to split your data into training and test sets, following a 75-25 split by default. One of its key features is its ability to ensure equal proportions of binary classes (0s and 1s) in both the `training` and `test` data sets, which helps maintain class balance for binary classification tasks.

Additionally, the function includes the return_data argument to give users flexibility in the output:

-   When `return_data` = `FALSE`, the function returns only the indices of the training data set.

-   When `return_data` = `TRUE`, the function returns a list containing two elements: the `training` data set and the `test` data set, both with balanced class proportions.

For this example, we will use a 80-20 split.

```{r}

split_data <- train_test_sampling(iris_binary, dependent_var="Virginica", train_prop = 0.8, 
                                  return_data=TRUE, seed=123)
# This will give us a list containing two data frames: train with 80% of the observations and test 
# with 20%.
# Extract training and test sets.
train_data <- split_data$train
test_data <- split_data$test

# Checking the split.
str(train_data)
str(test_data)

# Checking the dimensions
dim(iris_binary)
dim(train_data)
dim(test_data)
```

## Using `lr` to Train a Logistic Regression Model

The `lr` function trains a logistic regression model using numerical optimization. The goal is to find the coefficient vector `β` that minimizes the negative log-likelihood function:

$$
\hat{\beta} := \arg\min_\beta \sum_{i=1}^n (-y_i \cdot \ln(p_i) - (1 - y_i) \cdot \ln(1 - p_i)),
$$

where

$$
p_i := \frac{1}{1 + \exp(-x_i^T \beta)},
$$

and $y_i$ and $x_i$ represent the $i$th observation of the response and predictors, respectively.

The optimization starts with initial values for `β` obtained from the least-squares formula:

$$
\beta_0 = (X^T X)^{-1} X^T y
$$

The `lr` function uses numerical optimization (`optim` in R) to estimate the coefficients that minimize the negative log-likelihood. The initial values for optimization are calculated using the least-squares method, which provides a good starting point for the optimization algorithm. The optimization process iteratively adjusts the coefficients to find the minimum value of the negative log-likelihood.

To assess the uncertainty of the estimated coefficients, we can compute bootstrap confidence intervals. The user can specify the significance level $\alpha$ and the number of bootstrap samples (default is 20).

We use a cut-off value of 0.5 to classify the predictions. Based on this threshold, we generate a confusion matrix and calculate various performance metrics, including:

-   **Prevalence**: The proportion of positive cases.

-   **Accuracy**: The proportion of correctly classified cases.

-   **Sensitivity**: The true positive rate.

-   **Specificity**: The true negative rate.

-   **False Discovery Rate**: The proportion of false positives among the predicted positives.

-   **Diagnostic Odds Ratio**: The ratio of the odds of a positive result occurring in a true positive case to the odds of a positive result occurring in a false positive case.

-   We use `Sepal.Length` and `Sepal.Width` as predictors to train the model.

```{r}
# Train logistic regression model
model <- lr(Virginica ~ Sepal.Length + Sepal.Width, data = train_data, B=20, alpha=0.05)

# The model will provide important information like the coefficients of the logistic regression, 
# bootstrap confidence intervals, confusion matrix, prevalence, accuracy, sensitivity, specificity, etc.

# The model predicted 28 true positives, 10 false positives, 70 true negatives, and 12 false positives. 
model$confusion_matrix

# The accuracy of the model is 81.6%.
model$accuracy

# The was able to correctly identify the positives in 87.5% of the cases.
model$sensitivity

# The model was able to correctly identify 87.5% of the negatives. 
model$specificity

# All of these results correspond only to the training data set. 
```

## Using `predict_test` for Test Data Predictions

We can now evaluate the model's performance on the test data using `predict_test`.

```{r}
# Predict on test dataset
test_predictions <- predict_test(model=model, new_data=test_data, dependent_variable_col = "Virginica")
# The confusion matrix indicates that the model predicted 16 true negatives, 4 false negatives, 3 false 
# negatives and 7 true positives.

# Display predictions.
head(test_predictions)
```

## Using `predict_new` for New Data Predictions

Finally, let's use the `predict_new` function to generate predictions for new data points. We will create a small sample of new observations.

```{r}
# Creating new data points for prediction
new_data <- data.frame(Sepal.Length = c(5.1, 6.3),
                       Sepal.Width = c(3.5, 3.0))

# Predicting using the trained model
new_predictions <- predict_new(data=new_data, model=model, threshold=0.5 )

# Display predictions
new_predictions

# The model predicts that both new observations are not from the Virginica specie. 
```

## Summary

In this vignette, we demonstrated how to use the `TrainPredict` package to:

1.  Split a dataset into training and test sets using `train_test_sampling`.
2.  Train a logistic regression model using `lr`.
3.  Make predictions on test data using `predict_test`.
4.  Generate binary classifications for new data using `predict_new`.

These functions make it easy to perform end-to-end logistic regression modeling, from data preparation to model deployment.

## Resources

This package was developed with the aid of Large Language Models, including ChatGPT, which provided support in brainstorming, problem-solving, and enhancing documentation quality. The final outputs are the result of careful review and adaptation to the specific goals of this project.

## References

- Beckman, M., Guerrier, S., Lee, J., Molinari, R., Orso, S., & Rudnytskyi, I. (2020). *An Introduction to Statistical Programming Methods with R*. Retrieved from [https://smac-group.github.io/ds/](https://smac-group.github.io/ds/)

-   OpenAI. *Predict Outcome Function - ChatGPT Output*. Available at: <https://chatgpt.com/share/675212cd-f6bc-800d-9c79-9dc9a6946ef9>. Accessed: 2024-12-05.
-   OpenAI. *Logistic Regression with Bootstrapping - ChatGPT Output*. Available at: <https://chatgpt.com/share/675212cd-f6bc-800d-9c79-9dc9a6946ef9>. Accessed: 2024-12-01.
